ggplot(df_long, aes(x = Q1, fill = Category)) +
geom_bar(position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency")
# Create a stacked bar chart without reshaping the dataframe
ggplot(df, aes(x = Q1)) +
geom_bar(aes(fill = strong_dem), position = "stack", color = "black") +
geom_bar(aes(fill = not_strong_dem), position = "stack", color = "black") +
geom_bar(aes(fill = ind_close_dem), position = "stack", color = "black") +
geom_bar(aes(fill = ind), position = "stack", color = "black") +
geom_bar(aes(fill = ind_close_rep), position = "stack", color = "black") +
geom_bar(aes(fill = not_strong_rep), position = "stack", color = "black") +
geom_bar(aes(fill = strong_rep), position = "stack", color = "black") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("strong_dem" = "blue",
"not_strong_dem" = "lightblue",
"ind_close_dem" = "green",
"ind" = "yellow",
"ind_close_rep" = "orange",
"not_strong_rep" = "red",
"strong_rep" = "darkred"),
guide = "none")
ggplot(df, aes(x = Q1, fill = Q1)) +
geom_bar(aes(fill = strong_dem), position = "stack") +
geom_bar(aes(fill = not_strong_dem), position = "stack") +
geom_bar(aes(fill = ind_close_dem), position = "stack") +
geom_bar(aes(fill = ind), position = "stack") +
geom_bar(aes(fill = ind_close_rep), position = "stack") +
geom_bar(aes(fill = not_strong_rep), position = "stack") +
geom_bar(aes(fill = strong_rep), position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("strong_dem" = "blue",
"not_strong_dem" = "lightblue",
"ind_close_dem" = "green",
"ind" = "yellow",
"ind_close_rep" = "orange",
"not_strong_rep" = "red",
"strong_rep" = "darkred"),
guide = "none")
ggplot(df, aes(x = Q1)) +
geom_bar(aes(fill = "strong_dem"), data = df[df$strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_dem"), data = df[df$not_strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_dem"), data = df[df$ind_close_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind"), data = df[df$ind == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_rep"), data = df[df$ind_close_rep == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_rep"), data = df[df$not_strong_rep == 1,], position = "stack") +
geom_bar(aes(fill = "strong_rep"), data = df[df$strong_rep == 1,], position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("strong_dem" = "blue",
"not_strong_dem" = "lightblue",
"ind_close_dem" = "green",
"ind" = "yellow",
"ind_close_rep" = "orange",
"not_strong_rep" = "red",
"strong_rep" = "darkred"),
guide = "none")
# Create a stacked bar chart without reshaping the dataframe
ggplot(df, aes(x = Q1)) +
geom_bar(aes(fill = "strong_dem"), data = df[df$strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_dem"), data = df[df$not_strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_dem"), data = df[df$ind_close_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind"), data = df[df$ind == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_rep"), data = df[df$ind_close_rep == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_rep"), data = df[df$not_strong_rep == 1,], position = "stack") +
geom_bar(aes(fill = "strong_rep"), data = df[df$strong_rep == 1,], position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("strong_dem" = "blue",
"not_strong_dem" = "lightblue",
"ind_close_dem" = "green",
"ind" = "yellow",
"ind_close_rep" = "orange",
"not_strong_rep" = "red",
"strong_rep" = "darkred"),
guide = "none")
# Create a stacked bar chart without reshaping the dataframe
ggplot(df, aes(x = Q1, fill = partisan_score)) +
geom_bar(position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("0" = "blue",
"1" = "lightblue",
"2" = "green",
"3" = "yellow",
"4" = "orange",
"5" = "red",
"6" = "darkred"),
labels = c("Strong Democrat",
"Not very strong Democrat",
"Independent closer to Democrats",
"Independent",
"Independent closer to Republicans",
"Not very strong Republican",
"Strong Republican"))
# Create a stacked bar chart without reshaping the dataframe
ggplot(df, aes(x = Q1)) +
geom_bar(aes(fill = "strong_dem"), data = df[df$strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_dem"), data = df[df$not_strong_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_dem"), data = df[df$ind_close_dem == 1,], position = "stack") +
geom_bar(aes(fill = "ind"), data = df[df$ind == 1,], position = "stack") +
geom_bar(aes(fill = "ind_close_rep"), data = df[df$ind_close_rep == 1,], position = "stack") +
geom_bar(aes(fill = "not_strong_rep"), data = df[df$not_strong_rep == 1,], position = "stack") +
geom_bar(aes(fill = "strong_rep"), data = df[df$strong_rep == 1,], position = "stack") +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
scale_fill_manual(values = c("strong_dem" = "blue",
"not_strong_dem" = "lightblue",
"ind_close_dem" = "green",
"ind" = "yellow",
"ind_close_rep" = "orange",
"not_strong_rep" = "red",
"strong_rep" = "darkred"),
guide = "none")
# Create separate bar charts for each partisan leaning category using faceting
ggplot(df, aes(x = Q1, fill = Q1)) +
geom_bar() +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
facet_wrap(~partisan_score, scales = "free") +
scale_fill_manual(values = c("blue", "lightblue", "green", "yellow", "orange", "red", "darkred"))
ggplot(df, aes(x = Q1)) +
geom_bar() +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency")
# Create separate charts for each value of partisan_score using faceting
ggplot(df, aes(x = Q1)) +
geom_bar() +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
facet_wrap(~ partisan_score)
df_filtered <- df[!is.na(df$partisan_score) & df$partisan_score != "", ]
# Create separate charts for each value of partisan_score using faceting
ggplot(df_filtered, aes(x = Q1)) +
geom_bar() +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency") +
facet_wrap(~ partisan_score)
# distribution of advantage assessment
ggplot(df, aes(x = Q1)) +
geom_bar() +
labs(title = "Which side do you think has the advantage?",
x = "Responses",
y = "Frequency")
# Distribution of responses to Q4 as a bar chart
ggplot(df, aes(x = Q4)) +
geom_bar() +
labs(title = "Do you think the United States is doing too much, too little, or about the right amount to help Ukraine?",
x = "Responses",
y = "Frequency")
# Define the desired order of levels
desired_order <- c("a few months", "six months", "six months to a year", "over a year", "two to four years", "five years or more")
# Convert Q2 to a factor with the desired order
df$Q2 <- factor(df$Q2, levels = desired_order)
# Create the bar chart with the reordered levels
ggplot(df, aes(x = Q2)) +
geom_bar() +
labs(title = "From this point forward, how long do you expect the war in Ukraine to last?",
x = "Responses",
y = "Frequency")
#  Topic Modeling & Network Analysis
###################################################
setwd("/Users/madiraa/Desktop/columbia_poan/spring_2024/program_evaluation/p_set_2")
#Load in Libraries & Read in Data
install.packages("tm", "readxl", "dplyr", "tidytext", "stringr", "topicmodels", "wordcloud")
library(readxl)
library(dplyr)
library(tidytext)
library(tm)
library(stringr)
library(topicmodels)
library(wordcloud)
#data <- read_excel("GOP Debate.xlsx")
data <- read_excel("ProblemSet2_Data.xlsx")
install.packages("tm", "readxl", "dplyr", "tidytext", "stringr", "topicmodels", "wordcloud")
#data <- read_excel("GOP Debate.xlsx")
data <- read_excel("ProblemSet2_Data.xlsx")
##Explore Data & Select Candidate For Assignment
##Feel Free to explore polling data (aggregated on realclearpolitics)
#( https://www.realclearpolitics.com/epolls/2016/president/us/2016_republican_presidential_nomination-3823.html#polls )
head(data)
#datakasich<-data %>% filter(candidate== "John Kasich")
#datatrump<-data %>% filter(candidate== "Donald Trump")
datarubio<-data %>% filter(Candidate== "Marco Rubio")
table(data$Sentiment)
data$sentiment_binarypositive <- ifelse(data$Sentiment == "Positive", "Positive", "Negative & Neutral")
data$sentiment_binarynegative <- ifelse(data$Sentiment == "Negative", "Negative", "Positive & Neutral")
table(data$sentiment_binarynegative)
table(data$sentiment_binarypositive)
response_counts <- table(data$Sentiment)
response_proportions <- prop.table(response_counts)
result_table <- data.frame(response = names(response_proportions),
frequency = response_counts, proportion = response_proportions)
print(result_table)
### TODO filter for confidence in sentiment
response_counts <- table(datarubio$Sentiment)
response_proportions <- prop.table(response_counts)
result_table <- data.frame(response = names(response_proportions), frequency = response_counts, proportion = response_proportions)
print(result_table)
View(datarubio)
### filter for confidence in sentiment
# Filter data for observations with sentiment confidence >= 0.6
filtered_data <- datarubio[datarubio$Sentiment_Confidence >= 0.6,]
# Filter data for observations with sentiment confidence >= 0.6
filtered_data <- datarubio[datarubio$"Sentiment Confidence" >= 0.6,]
# Generate result table
response_counts <- table(filtered_data$Sentiment)
response_proportions <- prop.table(response_counts)
result_table <- data.frame(response = names(response_proportions),
frequency = response_counts,
proportion = response_proportions)
# Print the result table
print(result_table)
table(datarubio$Subject, datarubio$Sentiment)
table(datarubio$Sentiment, datarubio$`Retweet Count`)
datarubio$TopicSentiment<- paste(datarubio$Subject, datarubio$Sentiment)
Subject_Retweet <- datarubio %>%
group_by(Subject) %>%
summarize(total_retweets = sum(`Retweet Count`)) %>%
arrange(desc(total_retweets))
print(Subject_Retweet)
Sentiment_Retweet <- datarubio %>%
group_by(Sentiment) %>%
summarize(total_retweets = sum(`Retweet Count`)) %>%
arrange(desc(total_retweets))
print(Sentiment_Retweet)
View(filtered_data)
datarubio <- filtered_data[filtered_data$Sentiment_Confidence >= 0.4, ]
# filter data rubio down to observations where the sentiment confidence score is .4 or higher
datarubio <- filtered_data[filtered_data$"Sentiment Confidence" >= 0.4, ]
# Filter data for observations with sentiment confidence >= 0.6
filtered_data <- datarubio[datarubio$"Sentiment Confidence" >= 0.6,]
# filter data rubio down to observations where the sentiment confidence score is .4 or higher
datarubio <- filtered_data[filtered_data$"Subject Confidence" >= 0.4, ]
table(datarubio$Subject, datarubio$Sentiment)
table(datarubio$Sentiment, datarubio$`Retweet Count`)
datarubio$TopicSentiment<- paste(datarubio$Subject, datarubio$Sentiment)
#Incorporate Retweets to adjust for frequency of Topics & Sentiment
Subject_Retweet <- datarubio %>%
group_by(Subject) %>%
summarize(total_retweets = sum(`Retweet Count`)) %>%
arrange(desc(total_retweets))
print(Subject_Retweet)
Sentiment_Retweet <- datarubio %>%
group_by(Sentiment) %>%
summarize(total_retweets = sum(`Retweet Count`)) %>%
arrange(desc(total_retweets))
print(Sentiment_Retweet)
SentimentSubject_Retweet<- datarubio %>%
group_by(TopicSentiment) %>%
summarize(total_retweets = sum(`Retweet Count`)) %>%
arrange(desc(total_retweets))
print(SentimentSubject_Retweet)
word_counts <- data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%  # Remove stop words
count(word, sort = TRUE)
test<-head(word_counts, 200)
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%  # Remove stop words
count(word, sort = TRUE)
test2<-head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "Immigration", Retweet_Count > 0)
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "Immigration", "Retweet Count" > 0)
# Tokenize and count words, excluding stop words
word_counts <- filtered_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
test2 <- head(word_counts, 100)
View(test2)
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "Immigration", "Retweet Count" > 0)
View(filtered_data)
View(filtered_data)
filtered_data
# Save filtered_data to a CSV file
write.csv(filtered_data, file = "filtered_data.csv", row.names = FALSE)
View(datarubio)
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "Immigration", "Retweet Count" > 0)
# Tokenize and count words, excluding stop words
word_counts <- filtered_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "Immigration")
# Tokenize and count words, excluding stop words
word_counts <- filtered_data %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Tokenize and count words, excluding stop words
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
View(datarubio)
# Filter data for observations where Subject = "Immigration" and "Retweet Count" > 0
filtered_data <- datarubio %>%
filter(Subject == "None of the above")
# Tokenize and count words, excluding stop words
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Define additional words and numbers to filter out
words_to_filter <- c("rubio", "marco", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# Filter data for observations where Subject = "None of the above"
filtered_data <- datarubio %>%
filter(Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Define additional words and numbers to filter out
words_to_filter <- c("rubio", "marco", "hilary", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# Filter data for observations where Subject = "None of the above"
filtered_data <- datarubio %>%
filter(Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
words_to_filter <- c("rubio","carlyfiorina", "marco", "hilary", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# Filter data for observations where Subject = "None of the above"
filtered_data <- datarubio %>%
filter(Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
words_to_filter <- c("rubio","carlyfiorina", "marco", "hilary", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# Filter data for observations where Subject = "None of the above"
filtered_data <- datarubio %>%
filter(Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
words_to_filter <- c("rubio","carlyfiorina", "marco", "hilary", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# filter for rubio tweets that are both positive and in the none of the above subject category
filtered_data <- datarubio %>%
filter(Sentiment == "positive", Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
# Define additional words and numbers to filter out
words_to_filter <- c("rubio","carlyfiorina", "marco", "hilary", "hillary", "carson", "marcorubio", "gopdebate", "gopdebates", "trump", "carly", "fiorina", "christie", "rubios", "kasich", "huckabee", "walker", "cruz", "johnkasich", "christie")
# filter for rubio tweets that are both positive and in the none of the above subject category
filtered_data <- datarubio %>%
filter(Sentiment == "positive", Subject == "None of the above")
# Tokenize and count words, excluding stop words, specified words, and numbers
word_counts <- datarubio %>%
unnest_tokens(word, text) %>%
filter(!word %in% words_to_filter & !grepl("^\\d+$", word)) %>%  # Remove specified words and numbers
anti_join(stop_words) %>%
count(word, sort = TRUE)
# Get the top 100 words
test2 <- head(word_counts, 100)
# Create a word cloud
wordcloud(words = word_counts$word, freq = word_counts$n, scale = c(3, 0.5),
colors = brewer.pal(8, "Dark2"))
table(datarubio$Subject)
datarubiofox<-datarubio %>% filter(datarubio$Subject == "Abortion")
# Step 1: Create a corpus
corpus <- Corpus(VectorSource(data$text))
# Step 2: Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Step 3: Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)
# Step 4: Convert the document-term matrix to a matrix
mat <- as.matrix(dtm)
##Potentially Could Run PCA here to inform number of groups
# Step 5: Create an LDA model
lda_model <- LDA(mat, k = 5)  # 'k' is the number of topics, adjust as needed
# Step 6: Print the top terms for each topic
terms(lda_model, 10)  # Change '10' to the desired number of top terms per topic
# Step 5: Create an LDA model
lda_model <- LDA(mat, k = 2)  # 'k' is the number of topics, adjust as needed
# Step 6: Print the top terms for each topic
terms(lda_model, 6)  # Change '10' to the desired number of top terms per topic
datarubiofox<-datarubio %>% filter(datarubio$Subject == "Immigration")
# Step 1: Create a corpus
corpus <- Corpus(VectorSource(data$text))
# Step 2: Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Step 3: Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)
# Step 4: Convert the document-term matrix to a matrix
mat <- as.matrix(dtm)
##Potentially Could Run PCA here to inform number of groups
# Step 5: Create an LDA model
lda_model <- LDA(mat, k = 2)  # 'k' is the number of topics, adjust as needed
# Step 6: Print the top terms for each topic
terms(lda_model, 6)  # Change '10' to the desired number of top terms per topic
# Step 5: Create an LDA model
lda_model <- LDA(mat, k = 4)  # 'k' is the number of topics, adjust as needed
# Step 6: Print the top terms for each topic
terms(lda_model, 6)  # Change '10' to the desired number of top terms per topic
datarubiofox<-datarubio %>% filter(datarubio$Subject == "Religion")
# Step 1: Create a corpus
corpus <- Corpus(VectorSource(data$text))
# Step 2: Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Step 3: Create a document-term matrix
dtm <- DocumentTermMatrix(corpus)
# Step 4: Convert the document-term matrix to a matrix
mat <- as.matrix(dtm)
##Potentially Could Run PCA here to inform number of groups
# Step 5: Create an LDA model
lda_model <- LDA(mat, k = 4)  # 'k' is the number of topics, adjust as needed
# Step 6: Print the top terms for each topic
terms(lda_model, 6)  # Change '10' to the desired number of top terms per topic
#Polling data collected from  Public Policy Polling
table(data$Candidate, data$Poll_9.1.15)
table(data$Candidate, data$Poll_7.22.15)
table(data$Candidate, data$`Shift in Poll`)
